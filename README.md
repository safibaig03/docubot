# ğŸ§  Agentic RAG Chatbot with MCP (LangGraph + LangChain)

Agentic RAG Chatbot with MCP is a Retrieval-Augmented Generation (RAG) chatbot that answers user queries using uploaded documents. It is built with a custom agentic architecture using FastAPI and Streamlit, and follows a Model Context Protocol (MCP) for inter-agent communication.

---

## ğŸš€ Features

- âœ… **Multi-Format Document Support**: Natively parses and understands `.pdf`, `.docx`, `.pptx`, `.csv`, and `.txt` files.  
- ğŸ§  **Custom Agentic Architecture**:
  - **CoordinatorAgent (FastAPI)**: Orchestrates the entire workflow.  
  - **IngestionAgent**: Parses and splits documents.  
  - **RetrievalAgent**: Embeds chunks and performs semantic search.  
  - **LLMResponseAgent**: Synthesizes answers from context.  
- ğŸ”— **Model Context Protocol (MCP)** for structured, in-memory message passing.  
- ğŸ”„ **Multi-LLM Support**: Switch between Google Gemini, Groq (Llama 3.1), and Hugging Face models.  
- ğŸŒ **Streamlit UI** for file uploads, multi-turn Q&A, and viewing source citations.  

---

## ğŸ§± Tech Stack

| Layer            | Tool                                              |
|-----------------|---------------------------------------------------|
| LLM Providers    | Google Gemini, Groq, Hugging Face                |
| Embeddings       | `sentence-transformers/all-MiniLM-L6-v2`         |
| Vector DB        | ChromaDB                                          |
| Backend          | FastAPI                                           |
| Frontend         | Streamlit                                         |
| Protocol         | Custom MCP (Pydantic)                             |
| Package Manager  | uv                                                |

---

## ğŸ“ Project Structure

```

agentic-rag-chatbot/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ agents/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ ingestion_agent.py
â”‚ â”‚ â”œâ”€â”€ llm_response_agent.py
â”‚ â”‚ â””â”€â”€ retrieval_agent.py
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ main.py
â”‚ â””â”€â”€ mcp_models.py
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ app.py
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ start.bat


```

## ğŸ› ï¸ Setup Instructions

### 1. ğŸ“¦ Install Dependencies
Make sure you are in the project's root directory.

# Create and activate virtual environment

```
uv venv
.\.venv\Scripts\activate

```

# Install requirements

```
uv pip install -r requirements.txt

```


### 2. ğŸ”‘ Set API Keys
Create a .env file in the project's root directory. Add your API keys:

GOOGLE_API_KEY="Get your key from https://aistudio.google.com/app/apikey"
GROQ_API_KEY="Get your key from https://console.groq.com/keys"
HUGGINGFACE_API_KEY="Get your key from https://huggingface.co/settings/tokens"


### 3. ğŸ–¥ï¸ Launch the Application

You need to run the backend and frontend in two separate terminals. A start.bat script is included for convenience on Windows.

Using the script:

```
.\start.bat

```


Manual Method:

Terminal 1 (Backend):

```
uvicorn app.main:app --reload

```


Terminal 2 (Frontend):

```
streamlit run ui/app.py

```